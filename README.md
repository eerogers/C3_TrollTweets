# Russian Troll Tweet Identifier
#### After the 2016 US election, bot and troll accounts linked to Russiaâ€™s Internet Research Agency were suspended from Twitter, but around 200,000 of their tweets, with reconstructed account information, were released to the public as a dataset. A close look at this dataset, in comparison with election-related tweets from the same period of time, from accounts that were not subsequently removed from twitter, could yield information on how trolls from this specific Russian agency operated, and most importantly how trolls/bots, which are operating for similar nefarious purposes - perhaps on different topics, could be more effectively caught and removed from the website. In this project, the objective was to identify key features capable of isolating troll accounts from normal accounts.

## 1. Data
#### The data from the Russian tweets was reconstructed by a team at NBC News, including Ben Popken and EJ Fox. 
#### Their story on the data is available here: [Twitter Deleted 200,000 Russian Troll Tweets: Read Them Here](https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731)  
#### The complete dataset is here: [Troll Tweet Dataset](https://www.kaggle.com/vikasg/russian-troll-tweets)
#### This data was compared to personally-gathered data on "normal" tweets from the same period of time, on the topic of elections. This tweet data was gathered via the Twitter API, using tweet ids that had been pre-gathered for 2016 election relevence into a dataset available from the Harvard Dataverse: [2016 United States Presidential Election Tweet Ids](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN)
#### My initial processing of this dataset can be found here: [Data Wrangling 1](https://github.com/eerogers/C3_TrollTweets/blob/main/Data_Wrangling1.ipynb)

## 2. Tweet Collection Method
#### The Harvard 2016 Election Tweet Id datasets (specifically those titled "election-filter"1-6 were used in combination with the Twitter API to return information about the tweets associated with the given ids. It's perhaps notable that only about 2/5 of the tweet ids were able to return tweet information, due to the other associated Twitter accounts having been either suspended or otherwise deleted. Presumably, some of these un-obtainable tweets were the exact tweets housed in the NBC dataset of troll tweets, their accounts having been banned by Twitter.
#### My collection method was imperfect, as the datasets of tweet ids was understandably enormous. I ran API calls on roughly the first 5,000 tweets in each of six election-filter files, realizing they were in chronological order and that I should do something to break up the timeline. Given more time (and/or a time machine), I would go back and harvest tweet data either more completely or more randomly so that the tweet dates returned were spaced out better across the entire election timeline. In future studies into the troll tweet dataset, I will apply a more randomized or a more complete method with regard to the Harvard tweet id dataset, but for now, it's important to note that the "normal" tweets are imperfectly spaced in time. It is also worth noting that, due to the nature of my method and the nature of the banned Twitter accounts, there are more individual twitter users within the "normal" tweets dataset than are within the troll tweets dataset. This imbalance might be correctable given a larger and more random sampling of tweets from the election filters that allows for a smaller group of individual users to be isolated without further limiting the number of individual tweets available for analysis.
#### The Data Wrangling Jupyter Notebook specifically associated with my method of pulling tweets is available here: [Twitter API calls](https://github.com/eerogers/C3_TrollTweets/blob/main/Data_Wrangling2.ipynb)
#### Note: this notebook contains extremely long print-outs as I was checking my methods so it is quite difficult to read. Also, not all of my API calls are visible within the notebook, as many calls using a given filter's ids had to be manually re-run at 500-tweet intervals due to the Twitter API understandably limiting the number of tweets that can be pulled using this method at any one time.

## 3. Data Cleaning
#### Various specific problems arose in cleaning the thusly-compiled tweet data:
#### First, imbalance: there were over 200,000 individual troll tweets in the NBC dataset, yet only around 13,000 normal election tweets were able to be pulled within the timeline of this project. That problem was largely corrected by other necessary data cleaning steps, which ended up narrowing down the NBC dataset to around 50,000 tweets in the end. The more concerning imbalance overall (in my opinion), was the imbalance of unique user accounts described above in 'Tweet Collection Method.'
#### The first step, conducted primarily due to logistical issues, involved selecting down to only tweets in the English language. This was done by selecting the normal tweets' specified language and by selecting the troll tweets' associated user's specified language (it's unclear why, but the troll dataset did not have a language attached to most tweets, but did have one attached to most users, while the reverse was true of my API tweets). 
#### The second step was the main one in terms of narrowing down the troll tweet dataset, and it was based on a logical constraint of the Harvard datasets' parameters. The Harvard election filters were clearly run with parameters that defined a specific time period leading up to the election. Conversely, the troll tweet dataset appears to represent the entirety of the troll accounts' tweets, many extending to a time after the election and some preceding the election back into 2015. In order to bring the tweets more in line with each other, it was necessary to impose the same timeline limitations on the trolls tweets (by removing those that fell outside of the min and max dates associated with the final normal tweet dataset). Note: If more tweets are added into the normal tweet dataset in the future, specifically from the 6th election filter (which is the last), the max date will need to be updated accordingly, as more troll tweets will likely fit into the normal tweets' timeframe.
#### After thusly paring down the troll tweet dataset, it became clear that a quirk of this dataset was the existence of duplicate tweet ids, sometimes coresponding to different accounts that were tweeting the same text at the exact same time. I ultimately made the call to remove duplicates that were out-and-out duplicates (the same tweet from the same account) but I retained tweets that were duplicates of the same text and tweet id but using different accounts. I did this by creating a unique id for them that combined the tweet ID with the associated handle in a string. I also created a 'duplicated' column that took into account all of the tweet ids that existed in the dataset twice, because this felt like pertinent information to retain.
#### Finally, I filled NAs in the troll tweet dataset (which appeared to have been reconstructed but imperfectly) either by filling certain numeric NAs with the median values taken from all tweets in the combined dataset, or by more necessarily creative means in categories that were not numeric. In the case of missing source information, I did a check on the individual accounts that were missing that information (luckily each had created at least one other tweet that was not missing its source value), and then, in the instances where the account had used more than one different source, I filled the NA with the most frequently used source. In the instances where the account only used one source, I filled their missing source values with that value. There was also missing data in the mentions and url categories, which fortunately were easy to fill by applying some text-based scraping techniques to the associated tweet texts. 
#### I did some feature engineering to create the following categories: in_reply, which became a simple boolean of whether a tweet was in fact in reply to another user, 'creation_delta,' which is an int representing the different in days between a tweet's creation and an account's creation, 'duplicated,' which is the boolean category for duplicate tweets mentioned above, ''emoji_count,' which was created by applying spacymoji to the tweet text data in order to count the number of character in a tweet that were emojis, 'emoji_pct,' which applied this count to the tweet length in order to return a percentage representation of the breakdown, 'tweet_length,' which checked the tweet strings for their total length, 'desc_length,' which checked the user descriptions for their total length, 'mention_count,' which counted the number of mentions in a given tweet, 'hashtag_count,' which counted the number of hashtags in a given tweet, 'url_count,' which counted the number of urls in a given tweet, 'handle_length,' which counted the total character length of a user's handle, 'num_handle_pct,' which turned the number of handle character and a count of those character which were numeric in a percentage representation, 'url_avg_length,' which counted the average length of the urls attached to a tweet, and text_vecs and desc_vecs, which were document vectors built by applying spacy's doc.vec functionality to both processed tweets and processed user descriptions, with their emojis transformed into the emoji's text description). 
#### As a note: the text_vecs and desc_vecs categories were ultimately removed before running the final models because they were overly cumbersome. They also failed to shed light on the tweets using PCA, so their impact likely would have been negligible or detrimental.
#### As a final step, I did one-hot encoding on the domain extensions (e.g. '.com', '.edu', '.gov' etc.) of the tweets' urls, as well as one-hot encoding on the source of each tweet. I trimmed each of these categories so that any binary category was removed if it did not have at least 10 recorded '1's. (It's arguable that this left too many categories and I should have trimmed further.)
#### All of the data cleaning can be found across the remaining data wrangling jupyter notebooks: [Data Wrangling 3](https://github.com/eerogers/C3_TrollTweets/blob/main/Data_Wrangling_3.ipynb), [Data Wrangling 4](https://github.com/eerogers/C3_TrollTweets/blob/main/Data_Wrangling_4.ipynb), and [Data Wrangling 5](https://github.com/eerogers/C3_TrollTweets/blob/main/Data_Wrangling_5.ipynb)

## 4. EDA
#### In EDA, I was able to visualize how some of the tweet info broke down between troll tweets and normal tweets. This was done largely by employing histograms and violinplots, but a heatmap was also employed to visualize correlation.
#### One of the major takeaways from this visualization: the creation deltas of the troll tweet users tended to peak in the middle, suggesting these troll tweets tended to roll out in a pattern, at a certain delay from the time of account creation. The creation deltas associated with normal tweets behaved very differently in the histogram; however, it absolutely needs to me noted that there were only 261 unique troll accounts included in the final troll dataset, while there were 10,253 in the normal tweet dataset. This imbalance will have to be corrected in a future study to verify the importance of this divergence and other divergences which pertain to unique users rather than to unique tweets.
#### After creation_delta, which was the most important feature in the forthcoming random forest model, 'user_favorites_count', 'statuses_count', 'followers_count', 'retweet_count', 'friends_count', 'listed_count', 'source_name_twitter for iphone', 'source_name_twitter web client', 'source_name_twitter for android', 'mention_count', 'retweeted', 'desc_length' and 'num_handle_pct' were visualized with violinplots and/or histograms as the next most important features, in order. 
#### Many of these features, as you can see, do pertain to the user accounts, which suggests a need to better balance the dataset to account for a finite number of troll users contained therein. It is interesting to consider that 'retweet_count' ranks very highly while pertaining to unique tweets, not unique users. This could be because unique users' follower counts are still what they are, limiting the relative exposure of a tweet to whatever it might be. Having said that, it also appears that troll tweets are more likely than normal tweets to be retweets themselves, which potentially negates this theory. It is also interesting to note that the troll tweets were far more likely than normal tweets to use Twitter Web Client as their source, while being far less likely to use Twitter for iPhone and Twitter for Android than the normal user (of course, this is slightly offset by the limited number of users who are trolls, but I do think it stands to reason that a troll farm would have difficulty mimicking the true variety of sources present in the normal dataset and that this drawback would make their behavior somewhat predictable in this regard.) Finally, I found it interesting that these troll accounts actually did not have a preponderance of numbers in their handles, as one might expect them to. In fact, there was a slight reversal of expectation here, with most troll accounts having a very low percentage of numbers per character in their handle relative to the normal tweets. 
#### [EDA](https://github.com/eerogers/C3_TrollTweets/blob/main/EDA.ipynb)

## 5. Modeling
#### The first thing I wanted to attempt in the modeling stage was a PCA model that could capture the complexity of the text vectors attached to my data. This yielded disappointingly little. First, I attached the text and description vectors to my already sizeable series of columns, then I scaled my data, then I tried to apply PCA to pick out some meaningful way in which the complexity differed between troll and normal tweets. I didn't achieve much. The most complexity my model could describe was 24.5%, and even then it did not provide much visual explanation beyond several peripheral troll outliers around a big clump in the middle. This got somewhat better when I narrowed the data I was visualizing down to the description vectors and then the text vectors, individually, but still they only visualized 44.1% of the variance each. I then removed the tweets labeled as 'retweeted' do see if that would improve clarity on the text vectors. The result was a visualization of only 38.4% of the variance, though the outlying troll tweets to seem more visually clear and the clump in the middle seems smaller. My conclusion is that I will need to fine-tune how I'm approaching the text and turning it into numeric data if I hope to improve on this aspect of the model.
#### Following this conclusion, I returned to my tweet data and removed the unhelpful noise of text and description vectors before doing a train/test split and scaling all my data based on the training set. I then ran my data through a Random Forest Classifier, using a grid search to select the optimal number of estimators, an entropy-based Decision Tree, a gini-based Decision Tree, a Gradient Boosting Classifier and a Light Gradient Boosting Model optimized by Bayesian optimization. All models performed very well, with the LGBM and Random Forest both finishing around .999, even in the balanced accuracy score, which was the most important metric given the still-imbalanced nature of the dataset. Because the score of these two models are indeed so close to each other, I believe the deciding factor between them would be computation time and ease of use within the desired deployment setting.
#### Finally though improvements need to be made to correct imbalances in the data, I think these high accuracy numbers show that promise does exist for this study, in particular surrounding the behavior of troll users. From the heavy importance (and telling histogram breakdown) of their creation deltas, as well as the specific sources they use, their tendency to retweet more than average and their tendency to have less engagement and lower follower counts, it seems likely to me that these troll accounts both lack the ability to perfectly mimic the chaotic nature of actual twitter users and lack the ability to perfectly disguise that their accounts were created for a specific purpose.
#### [Modeling](https://github.com/eerogers/C3_TrollTweets/blob/main/Modeling.ipynb)

## 6. Composite File
#### The following is a composite jupyter notebook, containing EDA and Modeling steps, as well as commentary, for ease of perusal:
#### [Final Project](https://github.com/eerogers/C3_TrollTweets/blob/main/Final_Project_File.ipynb)